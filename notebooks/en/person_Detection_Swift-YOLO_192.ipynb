{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzVPUHhIQklD"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <h1>Welcom to SSCMA for Google Colab Training Example üî• </h1>\n",
        "  <a href=\"https://sensecraftma.seeed.cc/\" target=\"_blank\"><img width=\"20%\" src=\"https://files.seeedstudio.com/sscma/docs/images/SSCMA-Hero.png\"></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq_5d-_1QklF"
      },
      "source": [
        "## ‚ö†Ô∏è Important Notice\n",
        "\n",
        "Before you begin, please ensure:\n",
        "\n",
        "1. **Enable GPU Acceleration**: Click `Runtime` ‚Üí `Change runtime type` ‚Üí Select `GPU` in `Hardware accelerator`\n",
        "2. **Colab Version Requirement**: **You MUST use Colab version 2025.07** for compatibility. Other versions may cause unexpected errors.\n",
        "\n",
        "If you encounter version-related issues, you can run the following commands in the first code cell to check your environment:\n",
        "\n",
        "```python\n",
        "# Check if GPU is available\n",
        "!nvidia-smi\n",
        "\n",
        "# Check Python version\n",
        "!python --version\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZeN7PV0QklG"
      },
      "source": [
        "# person Detection - Swift-YOLO\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/person_Detection_Swift-YOLO_192.ipynb)\n",
        "\n",
        "**Version:** 1.0.0\n",
        "\n",
        "**Category:** Object Detection\n",
        "\n",
        "**Algorithm:** [Swift-YOLO](configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py)\n",
        "\n",
        "**Dataset:** [Person](https://universe.roboflow.com/hanzhou-7mktt/ssperson/dataset/7#)\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT: To download the dataset, you need to set your Roboflow API key.**\n",
        "\n",
        "1. Go to your [Roboflow Settings](https://app.roboflow.com/settings/api).\n",
        "2. Copy your private API key.\n",
        "3. In Colab, click the **key icon** on the **left sidebar** to open Secrets.\n",
        "4. Add a new secret named `ROBOFLOW_API_KEY` with your API key as the value.\n",
        "\n",
        "**Class:** `person`\n",
        "\n",
        "![person Detection](https://files.seeedstudio.com/sscma/static/detection_person.png)\n",
        "\n",
        "The model is a Swift-YOLO model trained on the person detection dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvEbWuQYQklG"
      },
      "source": [
        "## ‚öôÔ∏èPrerequisites\n",
        "### Setup SSCMA\n",
        "Clone the [repository](https://github.com/Seeed-Studio/ModelAssistant) and install the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aQhr9RFGQklG",
        "outputId": "d7985c24-ba91-481e-8397-49ca826ee1a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ethos-u-vela\n",
            "  Downloading ethos_u_vela-4.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting flatbuffers==24.3.25 (from ethos-u-vela)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ethos-u-vela) (2.0.2)\n",
            "Downloading ethos_u_vela-4.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Installing collected packages: flatbuffers, ethos-u-vela\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 25.12.19\n",
            "    Uninstalling flatbuffers-25.12.19:\n",
            "      Successfully uninstalled flatbuffers-25.12.19\n",
            "Successfully installed ethos-u-vela-4.5.0 flatbuffers-24.3.25\n",
            "Cloning into 'ModelAssistant'...\n",
            "remote: Enumerating objects: 15755, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 15755 (delta 7), reused 17 (delta 2), pack-reused 15719 (from 1)\u001b[K\n",
            "Receiving objects: 100% (15755/15755), 26.60 MiB | 22.76 MiB/s, done.\n",
            "Resolving deltas: 100% (9193/9193), done.\n",
            "/content/ModelAssistant\n",
            "Checking if CUDA available... \u001b[031mNot found!\u001b[m\n",
            "Please enable GPU Runtime\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!pip install ethos-u-vela\n",
        "!git clone https://github.com/Seeed-Studio/ModelAssistant.git -b 2.0.0  #clone the repo\n",
        "%cd ModelAssistant\n",
        "!. ./scripts/setup_colab.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAPjmb2DQklH"
      },
      "source": [
        "### Download the pretrain model weights file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlqP0PQ8QklH"
      },
      "outputs": [],
      "source": [
        "%mkdir -p person_Detection_Swift-YOLO_192\n",
        "!wget -c https://files.seeedstudio.com/sscma/model_zoo/detection/person/person_detection.pth -O person_Detection_Swift-YOLO_192/pretrain.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgcROIx4QklH"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NddD3ZmsQklH"
      },
      "outputs": [],
      "source": [
        "%mkdir -p person_Detection_Swift-YOLO_192\n",
        "!pip install roboflow\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get(\"ROBOFLOW_API_KEY\")\n",
        "from roboflow import download_dataset\n",
        "dataset = download_dataset(\"https://universe.roboflow.com/hanzhou-7mktt/ssperson/dataset/7\", \"coco\")\n",
        "dataset_path = dataset.location\n",
        "import shutil\n",
        "shutil.move(dataset_path, \"person_Detection_Swift-YOLO_192/dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbUfEIHxQklH"
      },
      "source": [
        "## üöÄTrain a model with SSCMA\n",
        "All the training parameters are in the `config.py` file, you can change the parameters to train your own model.\n",
        "\n",
        "Below are explanations of some common parameters. You can also refer to the [documentation](https://sensecraftma.seeed.cc/tutorials/config) for more details.\n",
        "- `data_root` - the datasets path.\n",
        "- `epochs`- the train epochs. **we use 10 epochs as an example**.\n",
        "- `batch_size` - the batch size.\n",
        "- `height` - the image height.\n",
        "- `width` - the image width.\n",
        "- `load_from` - the pretrained model path.\n",
        "- `num_classes` - the number of classes.\n",
        "\n",
        "You can overwrite the parameters in the `config.py` file by using the `--cfg-options` argument.\n",
        "```bash\n",
        "# Example\n",
        "sscma.train config.py --cfg-options data_root=./datasets/test_dataset epochs=10\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqOOmJQzQklI"
      },
      "outputs": [],
      "source": [
        "!sscma.train configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py \\\n",
        "--cfg-options  \\\n",
        "    work_dir=person_Detection_Swift-YOLO_192 \\\n",
        "    num_classes=1 \\\n",
        "    epochs=10  \\\n",
        "    height=192 \\\n",
        "    width=192 \\\n",
        "    data_root=person_Detection_Swift-YOLO_192/dataset/ \\\n",
        "    load_from=person_Detection_Swift-YOLO_192/pretrain.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMokf5J3QklI"
      },
      "source": [
        "## üì¶Export the model\n",
        "After training, you can export the model to the format for deployment. SSCMA supports exporting to ONNX, and TensorFlow Lite at present.\n",
        "You can also refer to the [documentation](https://sensecraftma.seeed.cc/tutorials/export/overview) for more details.\n",
        "\n",
        "```bash\n",
        "python3 tools/export.py \\\n",
        "    \"<CONFIG_FILE_PATH>\" \\\n",
        "    \"<CHECKPOINT_FILE_PATH>\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6DTqzoyQklI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "with open('person_Detection_Swift-YOLO_192/last_checkpoint', 'r') as f:\n",
        "\tos.environ['CHECKPOINT_FILE_PATH'] = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfF6AjedQklI"
      },
      "outputs": [],
      "source": [
        "!sscma.export configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py $CHECKPOINT_FILE_PATH --cfg-options  \\\n",
        "    work_dir=person_Detection_Swift-YOLO_192 \\\n",
        "    num_classes=1 \\\n",
        "    epochs=10  \\\n",
        "    height=192 \\\n",
        "    width=192 \\\n",
        "    data_root=person_Detection_Swift-YOLO_192/dataset/ \\\n",
        "    load_from=person_Detection_Swift-YOLO_192/pretrain.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMJWAyw4QklI"
      },
      "source": [
        "### üìùEvaluate the model\n",
        "After exporting the model, you can evaluate the model on the test dataset.\n",
        "You can also refer to the [documentation](https://sensecraftma.seeed.cc/tutorials/export/overview) for more details.\n",
        "\n",
        "\n",
        "```bash\n",
        "python3 tools/inference.py \\\n",
        "    \"<CONFIG_FILE_PATH>\" \\\n",
        "    \"<CHECKPOINT_FILE_PATH>\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXWbCvNEQklI"
      },
      "source": [
        "### Evaluate the PyTorch model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ME2MR_eQklI"
      },
      "outputs": [],
      "source": [
        "!sscma.inference configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py ${CHECKPOINT_FILE_PATH%.*}.pth \\\n",
        "--cfg-options  \\\n",
        "    work_dir=person_Detection_Swift-YOLO_192 \\\n",
        "    num_classes=1 \\\n",
        "    epochs=10  \\\n",
        "    height=192 \\\n",
        "    width=192 \\\n",
        "    data_root=person_Detection_Swift-YOLO_192/dataset/ \\\n",
        "    load_from=person_Detection_Swift-YOLO_192/pretrain.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0pYs00CQklI"
      },
      "source": [
        "### Evaluate the ONNX model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh7ytAiBQklI"
      },
      "outputs": [],
      "source": [
        "!sscma.inference configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py ${CHECKPOINT_FILE_PATH%.*}_float32.onnx \\\n",
        "--cfg-options  \\\n",
        "    work_dir=person_Detection_Swift-YOLO_192 \\\n",
        "    num_classes=1 \\\n",
        "    epochs=10  \\\n",
        "    height=192 \\\n",
        "    width=192 \\\n",
        "    data_root=person_Detection_Swift-YOLO_192/dataset/ \\\n",
        "    load_from=person_Detection_Swift-YOLO_192/pretrain.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUgIEq-TQklI"
      },
      "source": [
        "### Evaluate the TFLite FLOAT32 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIbOVIw_QklI"
      },
      "outputs": [],
      "source": [
        "!sscma.inference configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py ${CHECKPOINT_FILE_PATH%.*}_float32.tflite \\\n",
        "--cfg-options  \\\n",
        "    work_dir=person_Detection_Swift-YOLO_192 \\\n",
        "    num_classes=1 \\\n",
        "    epochs=10  \\\n",
        "    height=192 \\\n",
        "    width=192 \\\n",
        "    data_root=person_Detection_Swift-YOLO_192/dataset/ \\\n",
        "    load_from=person_Detection_Swift-YOLO_192/pretrain.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAxmgzECQklI"
      },
      "source": [
        "### Evaluate the TFLite INT8 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiibi5wMQklJ"
      },
      "outputs": [],
      "source": [
        "!sscma.inference configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py ${CHECKPOINT_FILE_PATH%.*}_int8.tflite \\\n",
        "--cfg-options  \\\n",
        "    work_dir=person_Detection_Swift-YOLO_192 \\\n",
        "    num_classes=1 \\\n",
        "    epochs=10  \\\n",
        "    height=192 \\\n",
        "    width=192 \\\n",
        "    data_root=person_Detection_Swift-YOLO_192/dataset/ \\\n",
        "    load_from=person_Detection_Swift-YOLO_192/pretrain.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBQZeAXcQklJ"
      },
      "source": [
        "## ü§ñ Deploy the model\n",
        "After model training, evaluation and export, you can deploy the model to your device. You can refer to [Documentation](https://sensecraftma.seeed.cc/deploy/overview) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvnCtFEpQklJ"
      },
      "outputs": [],
      "source": [
        "%ls -lh person_Detection_Swift-YOLO_192/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlc5ppfHQklJ"
      },
      "source": [
        "### Thanks for Trying Out SSCMA üéâ\n",
        "\n",
        "Congratulations, you have completed this tutorial. If you are interested in more application scenarios or our projects, please feel free to give [SSCMA](https://github.com/Seeed-Studio/ModelAssistant) a star ‚ú® on GitHub.\n",
        "\n",
        "If you have any questions about this tutorial, please also feel free to [submit an issue](https://github.com/Seeed-Studio/ModelAssistant/issues)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}